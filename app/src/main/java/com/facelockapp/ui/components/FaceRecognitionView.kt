package com.facelockapp.ui.components

import android.content.Context
import android.graphics.Bitmap
import android.graphics.Matrix
import androidx.camera.core.CameraSelector
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.core.Preview
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.view.PreviewView
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.runtime.*
import androidx.compose.ui.Modifier
import androidx.compose.ui.platform.LocalContext
import androidx.compose.ui.platform.LocalLifecycleOwner
import androidx.compose.ui.viewinterop.AndroidView
import androidx.lifecycle.Lifecycle
import androidx.lifecycle.LifecycleEventObserver
import androidx.core.content.ContextCompat
import android.util.Log
import com.google.common.util.concurrent.ListenableFuture
import com.google.mlkit.vision.common.InputImage
import com.google.mlkit.vision.face.Face
import com.google.mlkit.vision.face.FaceDetection
import com.google.mlkit.vision.face.FaceDetectorOptions
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.support.common.FileUtil
import org.tensorflow.lite.support.image.ImageProcessor
import org.tensorflow.lite.support.image.TensorImage
import org.tensorflow.lite.support.image.ops.Rot90Op
import android.media.Image
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.ReadOnlyBufferException
import java.util.concurrent.Executors
import java.util.concurrent.atomic.AtomicBoolean
import kotlin.coroutines.resume
import kotlin.coroutines.suspendCoroutine
import kotlin.math.pow
import kotlin.math.sqrt

data class FaceEmbedding(val features: FloatArray) {
    override fun equals(other: Any?): Boolean {
        if (this === other) return true
        if (javaClass != other?.javaClass) return false
        other as FaceEmbedding
        return features.contentEquals(other.features)
    }

    override fun hashCode(): Int = features.contentHashCode()
}

// ×”×¡×£ ×œ×–×™×”×•×™ - FaceNet ××™×™×¦×¨ embeddings ×× ×•×¨××œ×™× ×‘-L2
// Cosine Similarity ×§×¨×•×‘ ×œ-1.0 = ×¤× ×™× ×××•×ª×• ××“×
// Cosine Similarity ×§×¨×•×‘ ×œ-0.0 = ×× ×©×™× ×©×•× ×™×
// ×¢×‘×•×¨ MobileFaceNet, threshold ×©×œ 0.75-0.8 ×”×•× ×‘×˜×•×— ×™×•×ª×¨
private const val FACE_MATCH_THRESHOLD = 0.75f

// ××¡×¤×¨ ×“×’×™××•×ª ×œ×¨×™×©×•× - 5 ×–×” ××•×¤×˜×™××œ×™
private const val ENROLLMENT_REQUIRED_SAMPLES = 5

// ××¨×—×§ ××§×¡×™××œ×™ ×‘×™×Ÿ ×“×’×™××•×ª ×‘×–××Ÿ ×¨×™×©×•×
private const val ENROLLMENT_MAX_SAMPLE_DISTANCE = 0.8f

enum class FaceRecognitionState {
    WAITING_FOR_FACE,
    CHECKING_QUALITY,
    VERIFYING,
    FACE_NOT_MATCHED,
    FACE_MATCHED,
    NO_FACE_DETECTED,
    QUALITY_CHECK_FAILED,
    TOO_FAR,      // ×”××©×ª××© ×¨×—×•×§ ××“×™ ××”××¦×œ××”
    TOO_CLOSE     // ×”××©×ª××© ×§×¨×•×‘ ××“×™ ×œ××¦×œ××”
}

@Composable
fun FaceRecognitionView(
    isEnrollment: Boolean,
    onFaceEnrolled: ((FaceEmbedding) -> Unit)? = null,
    onFaceVerified: ((Boolean) -> Unit)? = null,
    storedEmbedding: FaceEmbedding? = null,
    onEnrollmentProgress: ((Int, Int) -> Unit)? = null,
    onStateChanged: ((FaceRecognitionState, String?) -> Unit)? = null
) {
    val context = LocalContext.current
    val lifecycleOwner = LocalLifecycleOwner.current
    val previewView = remember { 
        PreviewView(context).apply {
            // ×”×©×ª××© ×‘-COMPATIBLE (TextureView) ×‘××§×•× PERFORMANCE (SurfaceView) - ×–×” ×¢×•×‘×“ ×˜×•×‘ ×™×•×ª×¨ ×‘×ª×•×š scrollable containers
            implementationMode = PreviewView.ImplementationMode.COMPATIBLE
        }
    }
    var cameraProvider: ProcessCameraProvider? by remember { mutableStateOf(null) }
    var isLifecycleReady by remember { mutableStateOf(false) }
    
    // ×‘×“×•×§ ×©×”-lifecycle ××•×›×Ÿ (RESUMED) ×œ×¤× ×™ ×©××—×‘×¨×™× ××ª ×”××¦×œ××”
    DisposableEffect(lifecycleOwner) {
        val observer = LifecycleEventObserver { _, event ->
            isLifecycleReady = event == Lifecycle.Event.ON_RESUME || 
                             lifecycleOwner.lifecycle.currentState.isAtLeast(Lifecycle.State.RESUMED)
        }
        lifecycleOwner.lifecycle.addObserver(observer)
        onDispose {
            lifecycleOwner.lifecycle.removeObserver(observer)
        }
    }

    // Initialize FaceNet model
    val faceNetModel = remember {
        try {
            FaceNetModel(context)
        } catch (e: Exception) {
            null
        }
    }

    val faceDetector = remember {
        val options = FaceDetectorOptions.Builder()
            .setPerformanceMode(FaceDetectorOptions.PERFORMANCE_MODE_FAST) // Fast mode ××¡×¤×™×§
            .setLandmarkMode(FaceDetectorOptions.LANDMARK_MODE_NONE) // ×œ× ×¦×¨×™×š landmarks
            .setClassificationMode(FaceDetectorOptions.CLASSIFICATION_MODE_NONE)
            .setMinFaceSize(0.15f) // ×¤× ×™× ×§×˜× ×•×ª ×™×•×ª×¨
            .build()
        FaceDetection.getClient(options)
    }

    DisposableEffect(Unit) {
        onDispose {
            faceDetector.close()
            faceNetModel?.close()
        }
    }

    LaunchedEffect(Unit) {
        cameraProvider = ProcessCameraProvider.getInstance(context).await(context)
    }

    LaunchedEffect(cameraProvider, faceNetModel, isLifecycleReady) {
        val provider = cameraProvider ?: return@LaunchedEffect
        if (faceNetModel == null) {
            onStateChanged?.invoke(FaceRecognitionState.QUALITY_CHECK_FAILED, "×©×’×™××” ×‘×˜×¢×™× ×ª ××•×“×œ ×–×™×”×•×™")
            return@LaunchedEffect
        }
        
        // ×”××ª×Ÿ ×©×”-lifecycle ××•×›×Ÿ ×œ×¤× ×™ ×©××—×‘×¨×™× ××ª ×”××¦×œ××”
        if (!isLifecycleReady) {
            return@LaunchedEffect
        }
        
        // ×”××ª×Ÿ ×§×¦×ª ×›×“×™ ×©×”-PreviewView ×™×”×™×” ××•×›×Ÿ
        kotlinx.coroutines.delay(100)

        try {
            provider.unbindAll()

            val preview = Preview.Builder().build().also {
                it.setSurfaceProvider(previewView.surfaceProvider)
            }

            val imageAnalysis = ImageAnalysis.Builder()
                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
                .setTargetRotation(previewView.display.rotation)
                .build()

            var processingImage = AtomicBoolean(false)
            var lastProcessTime = 0L
            val enrollmentSamples = mutableListOf<FaceEmbedding>()
            
            // ×–××Ÿ ××™× ×™××œ×™ ×‘×™×Ÿ ×¢×™×‘×•×“×™× - 500ms ××•×¤×˜×™××œ×™
            val MIN_PROCESS_INTERVAL_MS = 500L
            
            imageAnalysis.setAnalyzer(Executors.newSingleThreadExecutor()) { imageProxy ->
                val currentTime = System.currentTimeMillis()
                val timeSinceLastProcess = currentTime - lastProcessTime
                
                val shouldProcess = !processingImage.get() && 
                                  (isEnrollment || timeSinceLastProcess >= MIN_PROCESS_INTERVAL_MS)
                
                if (shouldProcess) {
                    processingImage.set(true)
                    lastProcessTime = currentTime
                    
                    // ××œ × ×¢×“×›×Ÿ ××ª ×”××¦×‘ ×›××Ÿ - ×–×” ×™×§×¨×” ×‘-processImageWithFaceNet
                    // ×–×” ××•× ×¢ ×¢×“×›×•× ×™× ×ª×›×•×¤×™× ××“×™ ×©×œ ×”××¦×‘ ×•×××¤×©×¨ ×œ×”×¦×™×’ ×”×•×“×¢×•×ª ××¨×—×§
                    
                    processImageWithFaceNet(
                        imageProxy,
                        faceDetector,
                        faceNetModel,
                        isEnrollment,
                        storedEmbedding,
                        onStateChanged,
                        processingImage,
                        onEnrolled = { embedding ->
                            val finalized = accumulateEnrollmentSample(
                                embedding,
                                enrollmentSamples,
                                onProgress = onEnrollmentProgress
                            )
                            processingImage.set(false)
                            if (finalized != null) {
                                onFaceEnrolled?.invoke(finalized)
                            }
                        },
                        onVerified = { isMatch ->
                            onFaceVerified?.invoke(isMatch)
                            processingImage.set(false)
                        },
                        onError = { processingImage.set(false) },
                        onNoFace = { processingImage.set(false) }
                    )
                } else {
                    imageProxy.close()
                }
            }

            provider.bindToLifecycle(
                lifecycleOwner,
                CameraSelector.DEFAULT_FRONT_CAMERA,
                preview,
                imageAnalysis
            )

        } catch (e: Exception) {
            // Camera binding failed
        }
    }

    AndroidView({ previewView }, modifier = Modifier.fillMaxSize())
}

private suspend fun <T> ListenableFuture<T>.await(context: Context): T {
    return suspendCoroutine { continuation ->
        addListener({ continuation.resume(get()) }, ContextCompat.getMainExecutor(context))
    }
}

// FaceNet Model Wrapper
/**
 * ××—×œ×§×” ×œ×¢×‘×•×“×” ×¢× MobileFaceNet - ××•×“×œ ×œ×–×™×”×•×™ ×¤× ×™×
 * ×”××•×“×œ ×××™×¨ ×ª××•× ×ª ×¤× ×™× ×œ-embedding (192 ××¡×¤×¨×™×) ×©××™×™×¦×’×™× ××ª ×”×¤× ×™× ×‘××•×¤×Ÿ ×™×™×—×•×“×™
 */
class FaceNetModel(context: Context) {
    private val interpreter: Interpreter
    
    // MobileFaceNet parameters
    private val inputSize = 112  // MobileFaceNet expects 112x112 images
    val embeddingSize = 192  // MobileFaceNet outputs 192-dimensional embeddings
    
    // Statistics for normalization (calculated from training data)
    private val meanValues = floatArrayOf(127.5f, 127.5f, 127.5f)
    private val stdValues = floatArrayOf(128f, 128f, 128f)

    init {
        try {
            val modelFile = FileUtil.loadMappedFile(context, "mobilefacenet.tflite")
            
            val options = Interpreter.Options().apply {
                setNumThreads(4) // Use 4 threads for better performance
                setUseNNAPI(false) // Disable NNAPI for compatibility
            }
            
            interpreter = Interpreter(modelFile, options)
            
            // Verify model output size
            val outputTensor = interpreter.getOutputTensor(0)
            val actualSize = outputTensor.shape()[1]
            
            if (actualSize != embeddingSize) {
                throw IllegalStateException("Model output size ($actualSize) doesn't match expected ($embeddingSize)")
            }
            
        } catch (e: Exception) {
            throw RuntimeException("Failed to load MobileFaceNet model: ${e.message}", e)
        }
    }

    /**
     * ×××™×¨ ×ª××•× ×ª ×¤× ×™× ×œ-embedding
     * @param bitmap ×ª××•× ×ª ×”×¤× ×™× (×™×›×•×œ×” ×œ×”×™×•×ª ×‘×›×œ ×’×•×“×œ - ×ª×•×ª×× ××•×˜×•××˜×™×ª)
     * @return embedding ×©×œ 192 ××¡×¤×¨×™× ×”××™×™×¦×’×™× ××ª ×”×¤× ×™×
     */
    fun getEmbedding(bitmap: Bitmap): FloatArray {
        try {
            // Step 1: Resize image to 112x112
            val resized = Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)
            
            // Step 2: Convert to ByteBuffer (model input format)
            val input = convertBitmapToByteBuffer(resized)
            
            // Step 3: Run inference
            val output = Array(1) { FloatArray(embeddingSize) }
            interpreter.run(input, output)
            
            val embedding = output[0]
            
            // Step 4: L2 Normalize (critical for face recognition!)
            val normalized = l2Normalize(embedding)
            
            // Verify normalization (should be ~1.0)
            val norm = sqrt(normalized.sumOf { it.toDouble() * it.toDouble() }.toFloat())
            Log.d("FaceNetModel", "Generated embedding: size=${normalized.size}, L2 norm=$norm")
            
            return normalized
            
        } catch (e: Exception) {
            Log.e("FaceNetModel", "Failed to generate embedding", e)
            throw RuntimeException("Failed to generate face embedding: ${e.message}", e)
        }
    }

    /**
     * ×××™×¨ Bitmap ×œ-ByteBuffer ×‘×¤×•×¨××˜ ×©×”××•×“×œ ××¦×¤×” ×œ×•
     * Normalization: (pixel - mean) / std
     */
    private fun convertBitmapToByteBuffer(bitmap: Bitmap): ByteBuffer {
        val byteBuffer = ByteBuffer.allocateDirect(4 * inputSize * inputSize * 3)
        byteBuffer.order(ByteOrder.nativeOrder())
        
        val intValues = IntArray(inputSize * inputSize)
        bitmap.getPixels(intValues, 0, bitmap.width, 0, 0, bitmap.width, bitmap.height)
        
        var pixel = 0
        for (i in 0 until inputSize) {
            for (j in 0 until inputSize) {
                val value = intValues[pixel++]
                
                // Extract RGB channels
                val r = (value shr 16 and 0xFF).toFloat()
                val g = (value shr 8 and 0xFF).toFloat()
                val b = (value and 0xFF).toFloat()
                
                // Normalize: (pixel - mean) / std
                byteBuffer.putFloat((r - meanValues[0]) / stdValues[0])
                byteBuffer.putFloat((g - meanValues[1]) / stdValues[1])
                byteBuffer.putFloat((b - meanValues[2]) / stdValues[2])
            }
        }
        
        return byteBuffer
    }

    /**
     * L2 Normalization - ×”×•×¤×š ××ª ×”-embedding ×œ-unit vector
     * ×–×” ×§×¨×™×˜×™! ×‘×œ×™ ×–×” ×”×”×©×•×•××” ×œ× ×ª×¢×‘×•×“!
     */
    private fun l2Normalize(embedding: FloatArray): FloatArray {
        // Calculate L2 norm (magnitude of the vector)
        var sumSquares = 0.0
        for (value in embedding) {
            sumSquares += (value * value).toDouble()
        }
        val norm = sqrt(sumSquares).toFloat()
        
        if (norm == 0f) {
            return embedding
        }
        
        // Divide each element by the norm
        val normalized = FloatArray(embedding.size)
        for (i in embedding.indices) {
            normalized[i] = embedding[i] / norm
        }
        
        return normalized
    }

    /**
     * ×¡×•×’×¨ ××ª ×”××•×“×œ ×•××©×—×¨×¨ ××©××‘×™×
     */
    fun close() {
        try {
            interpreter.close()
        } catch (e: Exception) {
            // Ignore errors on close
        }
    }
}

private fun processImageWithFaceNet(
    imageProxy: ImageProxy,
    faceDetector: com.google.mlkit.vision.face.FaceDetector,
    faceNetModel: FaceNetModel,
    isEnrollment: Boolean,
    storedEmbedding: FaceEmbedding?,
    onStateChanged: ((FaceRecognitionState, String?) -> Unit)?,
    processingImageRef: AtomicBoolean,
    onEnrolled: (FaceEmbedding) -> Unit,
    onVerified: (Boolean) -> Unit,
    onError: () -> Unit,
    onNoFace: () -> Unit
) {
    val mediaImage = imageProxy.image
    if (mediaImage != null) {
        val image = InputImage.fromMediaImage(mediaImage, imageProxy.imageInfo.rotationDegrees)
        // ××œ × ×¢×“×›×Ÿ ××ª ×”××¦×‘ ×›××Ÿ - ×–×” ×’×•×¨× ×œ×¢×“×›×•× ×™× ×ª×›×•×¤×™× ××“×™
        
        faceDetector.process(image)
            .addOnSuccessListener { faces ->
                if (faces.isNotEmpty()) {
                    val face = faces.first()
                    
                    // ×‘×–××Ÿ enrollment, ×‘×“×•×§ ××¨×—×§ ×œ×¤× ×™ ×‘×“×™×§×ª ××™×›×•×ª
                    if (isEnrollment) {
                        val distanceCheck = checkFaceDistance(face, mediaImage.width, mediaImage.height)
                        Log.d("FaceRecognitionView", "Distance check: isOptimal=${distanceCheck.isOptimal}, state=${distanceCheck.state}, message=${distanceCheck.message}, avgRatio=${(face.boundingBox.width().toFloat() / mediaImage.width + face.boundingBox.height().toFloat() / mediaImage.height) / 2f}")
                        if (!distanceCheck.isOptimal) {
                            // ×¢×“×›×Ÿ ××ª ×”××¦×‘ ×¢× ×”×•×“×¢×ª ×”××¨×—×§
                            onStateChanged?.invoke(
                                distanceCheck.state ?: FaceRecognitionState.QUALITY_CHECK_FAILED,
                                distanceCheck.message
                            )
                            processingImageRef.set(false)
                            imageProxy.close()
                            return@addOnSuccessListener
                        }
                    } else {
                        // ×× ×–×” ×œ× enrollment, ×¢×“×›×Ÿ "××—×¤×© ×¤× ×™×" ×¨×§ ×× ××™×Ÿ ×¤× ×™× ××• ×× ×¦×¨×™×š
                        onStateChanged?.invoke(FaceRecognitionState.WAITING_FOR_FACE, "××—×¤×© ×¤× ×™×...")
                    }
                    
                    // ×‘×“×•×§ ××ª ××™×›×•×ª ×”×¤× ×™× (××¨×›×–, ×’×•×“×œ ××™× ×™××œ×™)
                    val qualityCheck = checkFaceQuality(face, mediaImage.width, mediaImage.height)
                    
                    if (!qualityCheck.isGood) {
                        // ×¢×“×›×Ÿ ××ª ×”××¦×‘ ×©×”×¤× ×™× ×œ× ×‘××¨×›×–/×œ× ×§×¨×•×‘×•×ª ××¡×¤×™×§
                        onStateChanged?.invoke(FaceRecognitionState.QUALITY_CHECK_FAILED, qualityCheck.reason)
                        // ××‘×œ ××œ ×ª×¢×¦×•×¨ - ×”××©×š ×œ×‘×“×•×§ ××ª ×”×¤× ×™× ×”×‘××•×ª
                        // ×× ×”×¤× ×™× ×™×—×–×¨×• ×œ××¨×›×–, ×”×§×•×“ ×™×–×”×” ××•×ª×Ÿ
                        processingImageRef.set(false) // ×©×—×¨×¨ ××ª ×”-flag ×›×“×™ ×©×”×§×•×“ ×™×•×›×œ ×œ× ×¡×•×ª ×©×•×‘
                        imageProxy.close()
                        return@addOnSuccessListener
                    }
                    
                    // ×× ×”×¤× ×™× ×‘××¨×›×– ×•×‘××™×›×•×ª ×˜×•×‘×”, × ×¡×” ×œ×–×”×•×ª ××•×ª×Ÿ
                    // ×¢×“×›×Ÿ ××ª ×”××¦×‘ ×¨×§ ×œ×¤× ×™ ×¢×™×‘×•×“ ××©××¢×•×ª×™
                    onStateChanged?.invoke(FaceRecognitionState.VERIFYING, "××¢×‘×“...")
                    
                    try {
                        // Convert ImageProxy to Bitmap and extract face
                        val fullBitmap = mediaImageToBitmap(mediaImage)
                        val rotated = rotateBitmap(fullBitmap, imageProxy.imageInfo.rotationDegrees)
                        val faceBitmap = extractFaceFromBitmap(rotated, face)
                        
                        if (faceBitmap == null || faceBitmap.isRecycled) {
                            onError()
                            imageProxy.close()
                            return@addOnSuccessListener
                        }
                        
                        // Get embedding from MobileFaceNet
                        val embedding = faceNetModel.getEmbedding(faceBitmap)
                        val faceEmbedding = FaceEmbedding(embedding)
                        
                        if (isEnrollment) {
                            onEnrolled(faceEmbedding)
                        } else {
                            if (storedEmbedding != null) {
                                // Compare using cosine similarity
                                val similarity = cosineSimilarity(faceEmbedding.features, storedEmbedding.features)
                                val isMatch = similarity > FACE_MATCH_THRESHOLD
                                
                                // Log detailed information for debugging
                                Log.i("FaceRecognitionView", "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
                                Log.i("FaceRecognitionView", "ğŸ” MOBILEFACENET VERIFICATION:")
                                Log.i("FaceRecognitionView", "   Cosine Similarity: $similarity")
                                Log.i("FaceRecognitionView", "   Threshold: $FACE_MATCH_THRESHOLD")
                                Log.i("FaceRecognitionView", "   Difference: ${similarity - FACE_MATCH_THRESHOLD}")
                                Log.i("FaceRecognitionView", "   Match: ${if (isMatch) "âœ… YES - SAME PERSON" else "âŒ NO - DIFFERENT PERSON"}")
                                Log.i("FaceRecognitionView", "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
                                
                                if (isMatch) {
                                    onStateChanged?.invoke(FaceRecognitionState.FACE_MATCHED, "×¤× ×™× ×ª×•×××•×ª! âœ…")
                                } else {
                                    onStateChanged?.invoke(FaceRecognitionState.FACE_NOT_MATCHED, "×”×¤× ×™× ×œ× ×ª×•×××•×ª âŒ")
                                }
                                
                                onVerified(isMatch)
                            } else {
                                Log.e("FaceRecognitionView", "âŒ No stored embedding for comparison!")
                                onVerified(false)
                            }
                        }
                    } catch (e: Exception) {
                        onError()
                    }
                } else {
                    // ×× ××™×Ÿ ×¤× ×™×, ×¢×“×›×Ÿ "××—×¤×© ×¤× ×™×..."
                    if (isEnrollment) {
                        onStateChanged?.invoke(FaceRecognitionState.WAITING_FOR_FACE, "××—×¤×© ×¤× ×™×...")
                    }
                    onNoFace()
                }
                imageProxy.close()
            }
            .addOnFailureListener { e ->
                onStateChanged?.invoke(FaceRecognitionState.QUALITY_CHECK_FAILED, "×©×’×™××” ×‘×–×™×”×•×™ ×¤× ×™×")
                onError()
                imageProxy.close()
            }
    } else {
        onStateChanged?.invoke(FaceRecognitionState.QUALITY_CHECK_FAILED, "×©×’×™××” ×‘×§×¨×™××ª ×ª××•× ×”")
        onError()
        imageProxy.close()
    }
}

private fun extractFaceBitmap(imageProxy: ImageProxy, face: Face): Bitmap {
    val mediaImage = imageProxy.image ?: throw IllegalStateException("Image is null")
    val bitmap = mediaImageToBitmap(mediaImage)
    val rotated = rotateBitmap(bitmap, imageProxy.imageInfo.rotationDegrees)
    return extractFaceFromBitmap(rotated, face)
}

private fun extractFaceFromBitmap(bitmap: Bitmap, face: Face): Bitmap {
    val boundingBox = face.boundingBox
    
    // Add 30% margin around face
    val margin = 0.3f
    val marginX = (boundingBox.width() * margin).toInt()
    val marginY = (boundingBox.height() * margin).toInt()
    
    val left = maxOf(0, boundingBox.left - marginX)
    val top = maxOf(0, boundingBox.top - marginY)
    val right = minOf(bitmap.width, boundingBox.right + marginX)
    val bottom = minOf(bitmap.height, boundingBox.bottom + marginY)
    
    val width = right - left
    val height = bottom - top
    
    return if (width > 0 && height > 0) {
        Bitmap.createBitmap(bitmap, left, top, width, height)
    } else {
        bitmap
    }
}

// ×”××¨×ª MediaImage (YUV_420_888) ×œ-Bitmap
// YUV_420_888 format: Y plane (full size), U and V planes (half size, interleaved)
private fun mediaImageToBitmap(mediaImage: Image): Bitmap {
    try {
        val width = mediaImage.width
        val height = mediaImage.height
        
        val yBuffer = mediaImage.planes[0].buffer
        val uBuffer = mediaImage.planes[1].buffer
        val vBuffer = mediaImage.planes[2].buffer
        
        val ySize = yBuffer.remaining()
        val uSize = uBuffer.remaining()
        val vSize = vBuffer.remaining()
        
        // YUV_420_888: Y plane is full size, U and V are half size
        // Convert to NV21: Y plane + interleaved VU (V first, then U)
        val nv21 = ByteArray(ySize + uSize + vSize)
        
        // Copy Y plane
        yBuffer.get(nv21, 0, ySize)
        
        // Interleave V and U planes (V first, then U) for NV21 format
        val uvSize = uSize + vSize
        val uvBuffer = ByteArray(uvSize)
        
        // Read V and U planes
        val vArray = ByteArray(vSize)
        val uArray = ByteArray(uSize)
        vBuffer.get(vArray)
        uBuffer.get(uArray)
        
        // Interleave: VU VU VU... (for NV21)
        var uvIndex = 0
        for (i in 0 until minOf(vSize, uSize)) {
            uvBuffer[uvIndex++] = vArray[i]
            uvBuffer[uvIndex++] = uArray[i]
        }
        
        // Copy remaining if sizes differ
        if (vSize > uSize) {
            System.arraycopy(vArray, uSize, uvBuffer, uvIndex, vSize - uSize)
        } else if (uSize > vSize) {
            System.arraycopy(uArray, vSize, uvBuffer, uvIndex, uSize - vSize)
        }
        
        // Copy interleaved UV to NV21 array
        System.arraycopy(uvBuffer, 0, nv21, ySize, uvSize)
        
        // Convert NV21 to JPEG and then to Bitmap
        val yuvImage = android.graphics.YuvImage(
            nv21,
            android.graphics.ImageFormat.NV21,
            width,
            height,
            null
        )
        
        val out = java.io.ByteArrayOutputStream()
        val success = yuvImage.compressToJpeg(
            android.graphics.Rect(0, 0, width, height),
            100,
            out
        )
        
        if (!success) {
            return Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)
        }
        
        val imageBytes = out.toByteArray()
        val bitmap = android.graphics.BitmapFactory.decodeByteArray(imageBytes, 0, imageBytes.size)
        
        if (bitmap == null) {
            return Bitmap.createBitmap(width, height, Bitmap.Config.ARGB_8888)
    }
        
        return bitmap
    } catch (e: Exception) {
        return Bitmap.createBitmap(mediaImage.width, mediaImage.height, Bitmap.Config.ARGB_8888)
    }
}

private fun rotateBitmap(bitmap: Bitmap, degrees: Int): Bitmap {
    if (degrees == 0) return bitmap
    val matrix = Matrix().apply { postRotate(degrees.toFloat()) }
    return Bitmap.createBitmap(bitmap, 0, 0, bitmap.width, bitmap.height, matrix, true)
    }

private data class FaceQualityResult(val isGood: Boolean, val reason: String)

private data class FaceDistanceResult(
    val isOptimal: Boolean,
    val state: FaceRecognitionState?,
    val message: String
)

/**
 * ×‘×•×“×§ ××ª ×”××¨×—×§ ×©×œ ×”×¤× ×™× ××”××¦×œ××” ×¢×œ ×‘×¡×™×¡ ×’×•×“×œ ×”-bounding box
 * ××¨×—×§ ××•×¤×˜×™××œ×™: 30-40 ×¡"× (×‘×¢×¨×š 30-50% ××¨×•×—×‘/×’×•×‘×” ×”×ª××•× ×”)
 * 
 * @param face ×”×¤× ×™× ×”××–×•×”×•×ª
 * @param imageWidth ×¨×•×—×‘ ×”×ª××•× ×”
 * @param imageHeight ×’×•×‘×” ×”×ª××•× ×”
 * @return FaceDistanceResult ×¢× ×”××¦×‘ ×•×”×”×•×“×¢×” ×”××ª××™××™×
 */
private fun checkFaceDistance(face: Face, imageWidth: Int, imageHeight: Int): FaceDistanceResult {
    val boundingBox = face.boundingBox
    val faceWidth = boundingBox.width()
    val faceHeight = boundingBox.height()
    
    // ×—×©×‘ ××ª ×”×™×—×¡ ×‘×™×Ÿ ×’×•×“×œ ×”×¤× ×™× ×œ×’×•×“×œ ×”×ª××•× ×”
    val faceWidthRatio = faceWidth.toFloat() / imageWidth
    val faceHeightRatio = faceHeight.toFloat() / imageHeight
    val avgRatio = (faceWidthRatio + faceHeightRatio) / 2f
    
    // ××¨×—×§ ××•×¤×˜×™××œ×™: 30-50% ××”×ª××•× ×” (×‘×¢×¨×š 30-40 ×¡"×)
    // ×¤× ×™× ×§×˜× ×•×ª ××“×™ (< 30%) = ×¨×—×•×§ ××“×™
    // ×¤× ×™× ×’×“×•×œ×•×ª ××“×™ (> 50%) = ×§×¨×•×‘ ××“×™
    // ×¤× ×™× ×‘×’×•×“×œ ×˜×•×‘ (30-50%) = ××¨×—×§ ××•×¤×˜×™××œ×™
    
    return when {
        avgRatio < 0.30f -> {
            FaceDistanceResult(
                isOptimal = false,
                state = FaceRecognitionState.TOO_FAR,
                message = "××—×¤×© ×¤× ×™×, ×™×© ×œ×”×ª×§×¨×‘ ×œ××¦×œ××”"
            )
        }
        avgRatio > 0.50f -> {
            FaceDistanceResult(
                isOptimal = false,
                state = FaceRecognitionState.TOO_CLOSE,
                message = "××—×¤×© ×¤× ×™×, ×™×© ×œ×”×¨×—×™×§ ××ª ×”××¦×œ××”"
            )
        }
        else -> {
            FaceDistanceResult(
                isOptimal = true,
                state = null,
                message = "××¨×—×§ ××¦×•×™×Ÿ!"
            )
        }
    }
}

private fun checkFaceQuality(face: Face, imageWidth: Int, imageHeight: Int): FaceQualityResult {
    val boundingBox = face.boundingBox
    val faceWidth = boundingBox.width()
    val faceHeight = boundingBox.height()
    
    // ×’×•×“×œ ××™× ×™××œ×™ - 15% ××”×ª××•× ×”
    val minFaceSize = minOf(imageWidth, imageHeight) * 0.15f
    val faceSize = sqrt((faceWidth * faceHeight).toFloat())
    if (faceSize < minFaceSize) {
        return FaceQualityResult(false, "×”×ª×§×¨×‘ ×œ××¦×œ××”")
}

    // ×‘××¨×›×– - 35% ××”×ª××•× ×”
    val faceCenterX = boundingBox.centerX()
    val faceCenterY = boundingBox.centerY()
    val imageCenterX = imageWidth / 2f
    val imageCenterY = imageHeight / 2f
    val centerDistanceX = kotlin.math.abs(faceCenterX - imageCenterX) / imageWidth
    val centerDistanceY = kotlin.math.abs(faceCenterY - imageCenterY) / imageHeight
    
    if (centerDistanceX > 0.35f || centerDistanceY > 0.35f) {
        return FaceQualityResult(false, "×”×–×– ××ª ×”×¤× ×™× ×œ××¨×›×–")
    }
    
    return FaceQualityResult(true, "××™×›×•×ª ×˜×•×‘×”")
}

// Cosine Similarity - ×”×“×¨×š ×”× ×›×•× ×” ×œ×”×©×•×•×ª FaceNet embeddings
private fun cosineSimilarity(embedding1: FaceEmbedding, embedding2: FaceEmbedding): Float {
    return cosineSimilarity(embedding1.features, embedding2.features)
}

/**
 * ××—×©×‘ Cosine Similarity ×‘×™×Ÿ ×©× ×™ embeddings
 * ×¢×¨×š ×§×¨×•×‘ ×œ-1.0 = ×¤× ×™× ×××•×ª×• ××“×
 * ×¢×¨×š ×§×¨×•×‘ ×œ-0.0 = ×× ×©×™× ×©×•× ×™×
 */
private fun cosineSimilarity(embedding1: FloatArray, embedding2: FloatArray): Float {
    if (embedding1.size != embedding2.size) {
        return 0f
    }
    
    // Dot product (after L2 normalization, this IS the cosine similarity)
    var dotProduct = 0f
    for (i in embedding1.indices) {
        dotProduct += embedding1[i] * embedding2[i]
    }
    
    return dotProduct
}

private fun accumulateEnrollmentSample(
    newSample: FaceEmbedding,
    samples: MutableList<FaceEmbedding>,
    onProgress: ((Int, Int) -> Unit)? = null
): FaceEmbedding? {
    if (samples.isNotEmpty()) {
        val last = samples.last()
        val similarity = cosineSimilarity(newSample.features, last.features)
        val distance = 1 - similarity
        
        if (distance > ENROLLMENT_MAX_SAMPLE_DISTANCE) {
            return null
        }
    }

    samples.add(newSample)
    onProgress?.invoke(samples.size, ENROLLMENT_REQUIRED_SAMPLES)

    if (samples.size >= ENROLLMENT_REQUIRED_SAMPLES) {
        val dim = samples.first().features.size
        val avg = FloatArray(dim)

        samples.forEach { sample ->
            for (i in 0 until dim) {
                avg[i] += sample.features[i]
            }
        }

        for (i in 0 until dim) {
            avg[i] /= samples.size.toFloat()
        }
        
        // L2 normalize the averaged embedding
        val norm = sqrt(avg.sumOf { it.toDouble() * it.toDouble() }.toFloat())
        for (i in avg.indices) {
            avg[i] = avg[i] / norm
        }

        samples.clear()
        return FaceEmbedding(avg)
    }

    return null
}
